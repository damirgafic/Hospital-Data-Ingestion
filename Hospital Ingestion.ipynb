{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84fbd2af-8d79-484b-a03c-7b880a378c52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from threading import Lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72578267-4254-4e7e-b8bd-7bb1669c63da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"batch_date\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16f45327-d3b4-47bd-8e40-c9ecbc991b48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "hospital_datasets = get_hospital_datasets()\n",
    "max_parallel = 8 #variable for threadpool\n",
    "user_input = dbutils.widgets.get(\"batch_date\")\n",
    "current_catalog = spark.sql(\"SELECT current_catalog() AS current_catalog\").collect()[0].current_catalog\n",
    "if user_input:\n",
    "    try:\n",
    "        dt = datetime.strptime(user_input, \"%Y-%m-%d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Date must be in YYYY-MM-DD format\")\n",
    "else:\n",
    "    dt = datetime.today()\n",
    "print(dt)\n",
    "print(current_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0cc8a4b-5226-41ca-b729-8ca4f808ec25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_hospital_datasets():\n",
    "    #Initialize api dataset\n",
    "    cms_url = \"https://data.cms.gov/provider-data/api/1/metastore/schemas/dataset/items\"\n",
    "    resp = requests.get(cms_url)\n",
    "    cms_data = resp.json()\n",
    "\n",
    "    hospital_datasets = [\n",
    "        ds for ds in cms_data\n",
    "        if \"Hospitals\" in str(ds.get(\"theme\", \"\")) or \"hospital\" in str(ds.get(\"title\", \"\")).lower()\n",
    "    ]\n",
    "\n",
    "    return hospital_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4e08f16-337a-410e-9ecb-10bd8b784f07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_csv_links(hospital_datasets):\n",
    "    csv_links = []\n",
    "    for dataset in hospital_datasets:\n",
    "        for dist in dataset.get(\"distribution\", []):\n",
    "            download_url = dist.get(\"downloadURL\")\n",
    "            if download_url and download_url.endswith(\".csv\"):\n",
    "                csv_links.append(download_url)\n",
    "    return csv_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bdbfc8d-b6c6-45c8-b25a-a45fa5428904",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_schemas():\n",
    "    spark.sql(\"CREATE SCHEMA IF NOT EXISTS metadata\")\n",
    "    spark.sql(\"CREATE SCHEMA IF NOT EXISTS bronze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e89e309-63bb-42d4-92d7-8e07e617b319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_hospital_meta_table():\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS metadata.hospital_meta (\n",
    "        table_name STRING,\n",
    "        download_url STRING,\n",
    "        last_update_date TIMESTAMP, -- modified date\n",
    "        batch_date TIMESTAMP, --  current date\n",
    "        file_count INT,\n",
    "        update_flag STRING -- if 'Y\" means we need to update the table, else 'N' means we don't need to update the table\n",
    "    ) USING DELTA \n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "193ff51f-e939-40c3-a7c6-4cde35d8cec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_snake_case(name):\n",
    "        name = re.sub(r'[\\s\\-]+', '_', name)\n",
    "        name = re.sub(r'([a-z0-9])([A-Z])', r'\\1_\\2', name)\n",
    "        name = re.sub(r'[^a-zA-Z0-9_]', '', name)\n",
    "        return name.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "002bca08-0975-4589-bc0a-047bbe371876",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Downloading data from api and store it as a delta file in the landing zone\n",
    "\n",
    "meta_update_records = [] # List of records to update in metadata table\n",
    "\n",
    "def download_data(url):\n",
    "    try:\n",
    "        file_name = url.split(\"/\")[-1]\n",
    "        if file_name.lower().endswith('.csv'):\n",
    "            file_name = file_name[:-4]\n",
    "\n",
    "        # Replace any '-' with '_'\n",
    "        file_name = file_name.replace(\"-\", \"_\")\n",
    "\n",
    "        dbfs_path = f\"dbfs:/tmp/{file_name}.csv\"\n",
    "\n",
    "        # Find modified date for this url\n",
    "        modified_date = None\n",
    "        for ds in hospital_datasets:\n",
    "            for dist in ds.get(\"distribution\", []):\n",
    "                if dist.get(\"downloadURL\") == url:\n",
    "                    modified_date = ds.get(\"modified\")\n",
    "                    break\n",
    "            if modified_date:\n",
    "                break\n",
    "\n",
    "        # Convert modified_date to datetime variable\n",
    "        if modified_date:\n",
    "            from datetime import datetime\n",
    "            try:\n",
    "                modified_date_dt = datetime.strptime(modified_date, \"%Y-%m-%d\")\n",
    "            except Exception:\n",
    "                modified_date_dt = None\n",
    "        else:\n",
    "            modified_date_dt = None\n",
    "\n",
    "        # Check metadata table for last update date\n",
    "        meta_table = f\"{current_catalog}.metadata.hospital_meta\"\n",
    "        table_name = \"_\".join(file_name.lower().split())\n",
    "        meta_df = spark.table(meta_table).filter(F.col(\"table_name\") == table_name)\n",
    "        meta_row = meta_df.select(\"last_update_date\").collect()\n",
    "        last_update_date = meta_row[0][\"last_update_date\"] if meta_row else None\n",
    "\n",
    "        # Only download if record doesn't exist or modified_date is greater\n",
    "        if last_update_date:\n",
    "            if not modified_date_dt or modified_date_dt <= last_update_date:\n",
    "                print(f\"Skipping {url} (not newer than last update)\")\n",
    "                return None\n",
    "\n",
    "        # Download directly to DBFS\n",
    "        dbutils.fs.cp(url, dbfs_path)\n",
    "\n",
    "        # Read CSV\n",
    "        df = spark.read.option(\"header\", \"true\").csv(dbfs_path)\n",
    "\n",
    "        # Convert to snake_case\n",
    "        new_columns = [to_snake_case(c) for c in df.columns]\n",
    "        df = df.toDF(*new_columns)\n",
    "\n",
    "        # Clean up the file name\n",
    "        cleaned_name = \"_\".join(file_name.lower().split())\n",
    "        delta_path = f\"/mnt/landing/hospital/{dt.year}/{dt.month}/{dt.day}/{cleaned_name}\"\n",
    "\n",
    "        # Save as Delta\n",
    "        count = df.count()\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "        # Collect metadata record for later batch update\n",
    "        record = (cleaned_name, url, modified_date, count)\n",
    "        if record not in meta_update_records:\n",
    "            meta_update_records.append(record)\n",
    "\n",
    "        print(f\"Saved {url} to {delta_path} and collected metadata\")\n",
    "        return meta_update_records\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d103540d-2ba9-4751-8996-f942471aada8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def update_metadata(meta_update_records):\n",
    "    from pyspark.sql.functions import lit, col, when, greatest, to_timestamp\n",
    "\n",
    "    meta_table = f\"{current_catalog}.metadata.hospital_meta\"\n",
    "\n",
    "    # meta_update_records: List of tuples (table_name, download_url, last_update_date, file_count)\n",
    "    # Add batch_date and update_flag\n",
    "    batch_date = dt\n",
    "    records = []\n",
    "    for rec in meta_update_records:\n",
    "        table_name, download_url, last_update_date, file_count = rec\n",
    "        records.append((\n",
    "            table_name,\n",
    "            download_url,\n",
    "            last_update_date,\n",
    "            batch_date,\n",
    "            file_count,\n",
    "            \"Y\"  # default update_flag to 'Y'\n",
    "        ))\n",
    "\n",
    "    columns = [\"table_name\", \"download_url\", \"last_update_date\", \"batch_date\", \"file_count\", \"update_flag\"]\n",
    "    meta_update_df = spark.createDataFrame(records, columns)\n",
    "    meta_update_df = meta_update_df.withColumn(\"last_update_date\", to_timestamp(col(\"last_update_date\")))\n",
    "\n",
    "    meta_update_df.createOrReplaceTempView(\"meta_update\")\n",
    "\n",
    "    # merge below updates metadata table as follows:\n",
    "    # - If a record with same table_name exists:\n",
    "    #   - If new last_update_date is more recent, update all fields and set update_flag to 'Y'\n",
    "    #   - If new last_update_date is the same or older, do not update any fields\n",
    "    # - If no record exists for table_name, insert a new row\n",
    "    spark.sql(f\"\"\"\n",
    "    MERGE INTO {meta_table} t\n",
    "    USING meta_update s\n",
    "    ON t.table_name = s.table_name\n",
    "    WHEN MATCHED AND s.last_update_date > t.last_update_date THEN\n",
    "      UPDATE SET\n",
    "        t.download_url = s.download_url,\n",
    "        t.last_update_date = s.last_update_date,\n",
    "        t.batch_date = s.batch_date,\n",
    "        t.file_count = s.file_count,\n",
    "        t.update_flag = 'Y'\n",
    "    WHEN NOT MATCHED THEN\n",
    "      INSERT (table_name, download_url, last_update_date, batch_date, file_count, update_flag)\n",
    "      VALUES (s.table_name, s.download_url, s.last_update_date, s.batch_date, s.file_count, s.update_flag)\n",
    "    \"\"\")\n",
    "    meta_update_records = [] #clear out the list after write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef58e767-8397-4111-b2ee-14764b4651df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def landing_to_bronze(landing_base, catalog, schema):\n",
    "    from pathlib import Path\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "    meta_table = f\"{current_catalog}.metadata.hospital_meta\"\n",
    "    landing_path = f\"{landing_base}/{dt.year}/{dt.month}/{dt.day}/\"\n",
    "\n",
    "    # Get tables with update_flag = 'Y'\n",
    "    flagged_tables_df = spark.sql(f\"SELECT table_name FROM {meta_table} WHERE update_flag = 'Y'\")\n",
    "    flagged_tables = [row['table_name'] for row in flagged_tables_df.collect()]\n",
    "    files = dbutils.fs.ls(landing_path)\n",
    "\n",
    "    loaded_count = 0\n",
    "    loaded_count_lock = Lock()\n",
    "\n",
    "    def process_folder(f):\n",
    "        nonlocal loaded_count\n",
    "        if f.isDir():\n",
    "            table_name = Path(f.name).stem.lower()\n",
    "            if table_name in flagged_tables:\n",
    "                delta_path = f.path\n",
    "                uc_table = f\"{catalog}.{schema}.{table_name}\"\n",
    "                df = spark.read.format(\"delta\").load(delta_path)\n",
    "                df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(uc_table)\n",
    "                # Set update_flag to 'N' after loading\n",
    "                spark.sql(f\"UPDATE {meta_table} SET update_flag = 'N' WHERE table_name = '{table_name}'\")\n",
    "                with loaded_count_lock:\n",
    "                    loaded_count += 1\n",
    "                    print(f\"Loaded {delta_path} into {uc_table} (Total loaded: {loaded_count})\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        executor.map(process_folder, files)\n",
    "\n",
    "    print(f\"Total tables loaded: {loaded_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8299a508-62e9-4096-b6ba-6a4a44a8103e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    #initializing: \n",
    "    csv_links = get_csv_links(hospital_datasets)\n",
    "    create_schemas()\n",
    "    create_hospital_meta_table()\n",
    "\n",
    "    # step 1 - download data \n",
    "    urls_to_download = csv_links\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_parallel) as executor:\n",
    "        executor.map(download_data, urls_to_download)\n",
    "\n",
    "    # step 2 - update metadata\n",
    "    if len(meta_update_records) > 0:\n",
    "        print(\"Updating metadata:\")\n",
    "        update_metadata(meta_update_records)\n",
    "\n",
    "    # step 3 - move data to bronze layer tbls\n",
    "    landing_base = \"/mnt/landing/hospital\"\n",
    "    schema = \"bronze\"\n",
    "    landing_to_bronze(landing_base, current_catalog, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02d8b704-fd60-43d0-9c92-17c33ce83484",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "783c6418-c48e-4363-9574-d9c2e37c13ba",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"download_url\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1761517187501}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display metadata\n",
    "display(spark.sql(f\"select * from {current_catalog}.metadata.hospital_meta\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01e47be8-f01a-41f1-ade8-7256ca6f9885",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# setting up situation where we have updated files\n",
    "spark.sql(f\"\"\"UPDATE {current_catalog}.metadata.hospital_meta\n",
    "SET last_update_date = timestamp('2020-12-01T00:00:00.000+00:00')\n",
    "WHERE last_update_date <= timestamp('2024-12-03T00:00:00.000+00:00')\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81937712-8797-4709-95ad-5b2a7d3526dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# only downloading files that have modified date newer than the last_updated_date in the metadata table for respective dataset\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a603b30c-c0f4-4a8f-9b8d-68014b3584ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Hospital Ingestion",
   "widgets": {
    "batch_date": {
     "currentValue": "",
     "nuid": "03a3c2c8-5b77-4a82-bf37-98342a675f6f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "batch_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "batch_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
